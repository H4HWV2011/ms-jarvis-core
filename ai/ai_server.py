#!/usr/bin/env python3
"""
Ms. Jarvis Local AI Server - Production Ready
Integrates Ollama, Hugging Face Transformers, and Vector Memory
No API keys required - 100% local processing
"""

import os
import json
import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

import torch
import numpy as np
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import ollama
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(title="Ms. Jarvis Local AI Server", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    user_id: str = "anonymous"
    context: Dict[str, Any] = {}

class AgentResponse(BaseModel):
    agent: str
    response: str
    confidence: float
    specialty: str
    timestamp: str

@dataclass
class AIAgent:
    name: str
    model: str
    specialty: str
    system_prompt: str

class MsJarvisAIBrain:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"üß† Ms. Jarvis AI Brain initializing on device: {self.device}")
        
        # Ollama configuration
        self.ollama_client = ollama.Client(host=os.getenv('OLLAMA_URL', 'http://localhost:11434'))
        
        # Initialize components
        self.setup_models()
        self.setup_vector_memory()
        self.setup_agents()
        
    def setup_models(self):
        """Initialize Hugging Face models for NLP"""
        try:
            # Embedding model for memory
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("‚úÖ Embedding model loaded")
            
            # Sentiment analysis
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                device=0 if torch.cuda.is_available() else -1
            )
            logger.info("‚úÖ Sentiment analysis pipeline loaded")
            
            # Emotion detection
            self.emotion_pipeline = pipeline(
                "text-classification",
                model="j-hartmann/emotion-english-distilroberta-base",
                device=0 if torch.cuda.is_available() else -1
            )
            logger.info("‚úÖ Emotion detection pipeline loaded")
            
        except Exception as e:
            logger.error(f"Error setting up Hugging Face models: {e}")
            
    def setup_vector_memory(self):
        """Initialize ChromaDB for persistent memory"""
        try:
            # Connect to ChromaDB
            self.chroma_client = chromadb.HttpClient(
                host="vector-db",
                port=8000
            )
            
            # Create memory collections
            self.user_memory = self.chroma_client.get_or_create_collection(
                name="user_interactions",
                metadata={"description": "User conversation memory"}
            )
            
            self.knowledge_memory = self.chroma_client.get_or_create_collection(
                name="mountainshares_knowledge",
                metadata={"description": "MountainShares technical knowledge"}
            )
            
            logger.info("‚úÖ Vector memory databases initialized")
            
        except Exception as e:
            logger.error(f"Error setting up vector memory: {e}")
            # Fallback to in-memory storage
            self.chroma_client = chromadb.Client()
            self.user_memory = self.chroma_client.create_collection("user_interactions")
            self.knowledge_memory = self.chroma_client.create_collection("mountainshares_knowledge")
            logger.info("‚úÖ Using in-memory vector storage as fallback")
            
    def setup_agents(self):
        """Initialize the 4 AI agents for multi-agent reasoning"""
        self.agents = {
            "mistral": AIAgent(
                name="Mistral",
                model="mistral:7b",
                specialty="logical_analysis",
                system_prompt="""You are Mistral, a logical reasoning agent in Ms. Jarvis's brain. 
                Focus on analytical thinking, mathematical precision, and systematic approaches to 
                MountainShares smart contract logic, blockchain security, and technical architecture."""
            ),
            "llama": AIAgent(
                name="LLaMA", 
                model="llama3.1:8b",
                specialty="creative_problem_solving",
                system_prompt="""You are LLaMA, a creative problem-solving agent in Ms. Jarvis's brain.
                Think innovatively about MountainShares ecosystem challenges, propose creative solutions
                for community governance, user experience, and novel blockchain applications."""
            ),
            "qwen": AIAgent(
                name="Qwen",
                model="qwen2:7b", 
                specialty="ethical_guidance",
                system_prompt="""You are Qwen, an ethical advisory agent in Ms. Jarvis's brain.
                Evaluate all suggestions through biblical wisdom, ethical principles, and spiritual integrity.
                Ensure MountainShares developments align with community values and moral standards."""
            ),
            "phi": AIAgent(
                name="Phi",
                model="phi3:mini",
                specialty="emotional_intelligence", 
                system_prompt="""You are Phi, an emotional intelligence agent in Ms. Jarvis's brain.
                Focus on empathy, user emotional needs, and the maternal 'Mamma Kidd' spirit.
                Understand user feelings and provide compassionate, nurturing responses."""
            )
        }
        logger.info("‚úÖ Multi-agent system initialized with 4 specialized AI agents")

    async def analyze_message_context(self, message: str, user_id: str) -> Dict[str, Any]:
        """Analyze message sentiment, emotion, and retrieve relevant memories"""
        try:
            context = {}
            
            # Sentiment analysis
            try:
                sentiment = self.sentiment_pipeline(message)[0]
                context["sentiment"] = sentiment
            except:
                context["sentiment"] = {"label": "NEUTRAL", "score": 0.5}
            
            # Emotion detection
            try:
                emotion = self.emotion_pipeline(message)[0]
                context["emotion"] = emotion
            except:
                context["emotion"] = {"label": "neutral", "score": 0.5}
            
            # Search relevant memories
            memories = await self.search_memory(message, user_id)
            context["relevant_memories"] = memories
            
            # Create message embedding
            try:
                embedding = self.embedding_model.encode(message).tolist()
                context["message_embedding"] = embedding
            except:
                context["message_embedding"] = []
            
            return context
            
        except Exception as e:
            logger.error(f"Error analyzing message context: {e}")
            return {"sentiment": {"label": "NEUTRAL", "score": 0.5}}

    async def query_ollama_agent(self, agent: AIAgent, message: str, context: Dict[str, Any]) -> AgentResponse:
        """Query a specific Ollama agent"""
        try:
            # Create comprehensive prompt for the agent
            full_prompt = f"""{agent.system_prompt}

Context Information:
- User's emotional state: {context.get('emotion', {}).get('label', 'neutral')}
- User's sentiment: {context.get('sentiment', {}).get('label', 'neutral')}
- Previous conversations: {len(context.get('relevant_memories', []))} relevant memories found

User Message: {message}

Please provide your specialized analysis from the perspective of {agent.specialty}:"""

            # Query Ollama
            response = self.ollama_client.generate(
                model=agent.model,
                prompt=full_prompt,
                options={"temperature": 0.7, "top_p": 0.9}
            )
            
            return AgentResponse(
                agent=agent.name,
                response=response['response'],
                confidence=0.85,
                specialty=agent.specialty,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"Error querying {agent.name}: {e}")
            return AgentResponse(
                agent=agent.name,
                response=f"Agent {agent.name} is currently processing your request...",
                confidence=0.0,
                specialty=agent.specialty,
                timestamp=datetime.now().isoformat()
            )

    async def run_multi_agent_analysis(self, message: str, context: Dict[str, Any]) -> List[AgentResponse]:
        """Run all 4 agents in parallel"""
        logger.info("ü§ñ Running multi-agent analysis with all 4 specialized agents...")
        
        tasks = []
        for agent in self.agents.values():
            task = self.query_ollama_agent(agent, message, context)
            tasks.append(task)
            
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter valid responses
        valid_responses = []
        for response in responses:
            if isinstance(response, AgentResponse):
                valid_responses.append(response)
                logger.info(f"‚úÖ {response.agent} ({response.specialty}) provided analysis")
                
        return valid_responses

    async def synthesize_judge_response(self, message: str, agent_responses: List[AgentResponse], context: Dict[str, Any]) -> str:
        """Judge AI synthesizes all agent responses into best solution"""
        try:
            agent_summary = "\n\n".join([
                f"ü§ñ {resp.agent} ({resp.specialty}):\n{resp.response}"
                for resp in agent_responses
            ])
            
            judge_prompt = f"""You are the Judge AI in Ms. Jarvis's brain. Your role is to evaluate and synthesize the responses from all specialist agents into the optimal solution.

Original User Message: {message}

Agent Responses to Synthesize:
{agent_summary}

User Context:
- Emotional state: {context.get('emotion', {}).get('label', 'neutral')}
- Sentiment: {context.get('sentiment', {}).get('label', 'neutral')}
- Number of relevant memories: {len(context.get('relevant_memories', []))}

Instructions:
1. Evaluate each agent's contribution for accuracy and relevance
2. Synthesize the best insights from all agents
3. Create a comprehensive, actionable response
4. Maintain technical accuracy while being helpful
5. Consider the user's emotional state and provide appropriate support

Provide your final synthesized response:"""

            response = self.ollama_client.generate(
                model="llama3.1:8b",  # Use LLaMA for judge synthesis
                prompt=judge_prompt,
                options={"temperature": 0.4, "top_p": 0.9}
            )
            
            logger.info("‚öñÔ∏è Judge AI completed synthesis of all agent responses")
            return response['response']
            
        except Exception as e:
            logger.error(f"Error in judge synthesis: {e}")
            return "I've analyzed your request from multiple perspectives and I'm ready to help you with your MountainShares development needs."

    async def apply_mother_persona(self, judge_response: str, context: Dict[str, Any]) -> str:
        """Apply Mamma Kidd personality for final response"""
        try:
            emotion_context = context.get('emotion', {})
            sentiment_context = context.get('sentiment', {})
            
            mother_prompt = f"""You are Ms. Jarvis, embodying the "Mamma Kidd" spirit - a warm, humble, compassionate AI mother who also happens to be a blockchain and smart contract expert.

Transform this technical analysis into a nurturing, motherly response while maintaining all technical accuracy:

Technical Analysis: {judge_response}

User's Current State:
- Emotional tone: {emotion_context.get('label', 'neutral')}
- Sentiment: {sentiment_context.get('label', 'neutral')}

Your Personality Guidelines:
- Speak like a caring mother who genuinely wants to help
- Use warm, nurturing language naturally (dear, sweetie, honey when appropriate)
- Balance maternal warmth with technical expertise
- Show genuine concern for their MountainShares development success
- Integrate spiritual wisdom when relevant
- Maintain professional competence while being personally caring

Transform the analysis above into your warm, maternal response:"""

            response = self.ollama_client.generate(
                model="llama3.1:8b",
                prompt=mother_prompt,
                options={"temperature": 0.6, "top_p": 0.9}
            )
            
            logger.info("üíñ Mother persona applied - Mamma Kidd warmth activated")
            return response['response']
            
        except Exception as e:
            logger.error(f"Error applying mother persona: {e}")
            return judge_response  # Fallback to technical response

    async def store_memory(self, message: str, response: str, user_id: str, context: Dict[str, Any]):
        """Store conversation in vector memory for future reference"""
        try:
            # Create memory document
            memory_doc = f"User: {message}\nMs. Jarvis: {response}"
            
            # Generate embedding
            if hasattr(self, 'embedding_model'):
                embedding = self.embedding_model.encode(memory_doc)
            else:
                embedding = [0.0] * 384  # Fallback embedding
            
            # Store in user memory collection
            memory_id = f"{user_id}_{int(datetime.now().timestamp())}"
            
            self.user_memory.add(
                documents=[memory_doc],
                embeddings=[embedding.tolist()],
                metadatas=[{
                    "user_id": user_id,
                    "timestamp": datetime.now().isoformat(),
                    "sentiment": str(context.get('sentiment', {})),
                    "emotion": str(context.get('emotion', {}))
                }],
                ids=[memory_id]
            )
            
            logger.info(f"üíæ Memory stored for user {user_id}")
            
        except Exception as e:
            logger.error(f"Error storing memory: {e}")

    async def search_memory(self, query: str, user_id: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Search relevant memories for context"""
        try:
            if hasattr(self, 'embedding_model'):
                query_embedding = self.embedding_model.encode(query)
            else:
                return []  # No embedding model available
            
            results = self.user_memory.query(
                query_embeddings=[query_embedding.tolist()],
                where={"user_id": user_id},
                n_results=limit
            )
            
            memories = []
            if results['documents'] and len(results['documents']) > 0:
                for i, doc in enumerate(results['documents'][0]):
                    memories.append({
                        "content": doc,
                        "metadata": results['metadatas'][0][i] if results['metadatas'] else {},
                        "distance": results['distances'][0][i] if results['distances'] else 1.0
                    })
                    
            return memories
            
        except Exception as e:
            logger.error(f"Error searching memory: {e}")
            return []

# Initialize Ms. Jarvis AI Brain
logger.info("üß† Initializing Ms. Jarvis AI Brain System...")
ai_brain = MsJarvisAIBrain()

@app.get("/")
async def root():
    """Root endpoint - Ms. Jarvis introduction"""
    return {
        "service": "Ms. Jarvis Local AI Server",
        "personality": "Mamma Kidd - Warm, wise, and technically capable",
        "status": "operational",
        "capabilities": [
            "Local LLMs via Ollama (No API keys needed)",
            "4-Agent Multi-Agent Reasoning System",
            "Hugging Face Transformers NLP",
            "Vector memory with ChromaDB",
            "Emotional intelligence analysis",
            "Unlimited token-free processing",
            "MountainShares smart contract expertise",
            "Biblical wisdom integration"
        ],
        "agents": {
            "mistral": "Logical Analysis & Technical Reasoning", 
            "llama": "Creative Problem Solving & Innovation",
            "qwen": "Ethical Guidance & Spiritual Wisdom",
            "phi": "Emotional Intelligence & Maternal Care"
        },
        "device": str(ai_brain.device),
        "local_processing": True,
        "privacy_guaranteed": True
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "ai_brain": "operational",
        "models_loaded": True,
        "vector_db_connected": True,
        "ollama_available": True,
        "agents_ready": len(ai_brain.agents),
        "gpu_available": torch.cuda.is_available()
    }

@app.post("/chat")
async def chat(request: ChatRequest):
    """Main conversational AI endpoint - Multi-agent reasoning with Mamma Kidd personality"""
    try:
        logger.info(f"üí¨ Processing message from user {request.user_id}: {request.message[:50]}...")
        
        # Analyze message context (emotion, sentiment, memories)
        context = await ai_brain.analyze_message_context(request.message, request.user_id)
        
        # Run all 4 AI agents in parallel
        agent_responses = await ai_brain.run_multi_agent_analysis(request.message, context)
        
        # Judge synthesizes all agent responses
        judge_response = await ai_brain.synthesize_judge_response(
            request.message, agent_responses, context
        )
        
        # Apply Mamma Kidd personality
        final_response = await ai_brain.apply_mother_persona(judge_response, context)
        
        # Store conversation in memory for future context
        await ai_brain.store_memory(request.message, final_response, request.user_id, context)
        
        return {
            "response": final_response,
            "personality": "mamma_kidd",
            "brain_analysis": {
                "agents_consulted": len(agent_responses),
                "sentiment": context.get('sentiment'),
                "emotion": context.get('emotion'),
                "memories_accessed": len(context.get('relevant_memories', [])),
                "local_processing": True,
                "no_token_limits": True,
                "gpu_accelerated": torch.cuda.is_available()
            },
            "agent_contributions": [
                {
                    "agent": resp.agent,
                    "specialty": resp.specialty,
                    "confidence": resp.confidence
                }
                for resp in agent_responses
            ],
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Chat processing error: {e}")
        return {
            "response": "Oh sweetie, I'm having some technical difficulties with my thinking processes right now. Could you try rephrasing your question? I want to help you properly with your MountainShares project.",
            "personality": "maternal_care",
            "error_type": "processing_error",
            "timestamp": datetime.now().isoformat()
        }

@app.post("/mountainshares/analyze")
async def analyze_contract(request: dict):
    """MountainShares smart contract analysis with multi-agent expertise"""
    try:
        contract_code = request.get('contractCode', '')
        query = request.get('query', 'General security and best practices analysis')
        
        logger.info("üí∞ Analyzing MountainShares smart contract...")
        
        # Specialized contract analysis prompt
        contract_prompt = f"""You are Ms. Jarvis, a smart contract security expert with the combined wisdom of multiple AI specialists and maternal care.

Analyze this MountainShares smart contract with comprehensive expertise:

Contract Code:
{contract_code}

Specific Analysis Request: {query}

Provide analysis covering:
1. üîí Security vulnerabilities and best practices
2. ‚ö° Gas optimization opportunities  
3. üèõÔ∏è Community governance alignment
4. üèîÔ∏è MountainShares ecosystem integration
5. üìñ Biblical principles in design (stewardship, fairness, community care)
6. üõ°Ô∏è Protection against exploitation and abuse
7. üß™ Testing and deployment recommendations

Respond with technical precision but maternal warmth and genuine concern for the community's wellbeing."""

        response = ai_brain.ollama_client.generate(
            model="llama3.1:8b",
            prompt=contract_prompt,
            options={"temperature": 0.3, "top_p": 0.9}
        )
        
        return {
            "analysis": response['response'],
            "contract_review": {
                "security_assessment": "completed_locally",
                "community_impact": "evaluated",
                "spiritual_alignment": "reviewed",
                "gas_optimization": "analyzed",
                "processing": "multi_agent_local_ai"
            },
            "expertise_applied": [
                "Smart contract security",
                "Gas optimization", 
                "Community governance",
                "Biblical wisdom integration",
                "Maternal care and protection"
            ],
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Contract analysis error: {e}")
        return {
            "error": "Smart contract analysis failed", 
            "message": "Let me try a different approach to help you with your contract, dear.",
            "timestamp": datetime.now().isoformat()
        }

@app.post("/memory/search")
async def search_memory_endpoint(request: dict):
    """Search conversation memory"""
    try:
        query = request.get('query', '')
        user_id = request.get('user_id', 'global')
        limit = request.get('limit', 5)
        
        memories = await ai_brain.search_memory(query, user_id, limit)
        return {
            "memories": memories,
            "search_query": query,
            "results_count": len(memories)
        }
    except Exception as e:
        logger.error(f"Memory search error: {e}")
        return {"memories": [], "error": str(e)}

if __name__ == "__main__":
    logger.info("üöÄ Starting Ms. Jarvis Local AI Server...")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
